model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
  - model_name: gpt-4
    litellm_params:
      model: gpt-4
  - model_name: phi
    litellm_params:
      model: ollama/phi
  - model_name: dolphin-phi
    litellm_params:
      model: ollama/dolphin-phi
litellm_params:
  drop_params: True
  fallbacks: [{"phi": ["gpt-3.5-turbo"], "dolphin-phi": ["gpt-3.5-turbo"]}]


router_settings:
    model_group_alias: {
      "gpt3": "gpt-3.5-turbo",
      "gpt-3": "gpt-3.5-turbo",
      "gpt4": "gpt-4"} # all requests with `gpt-4` will be routed to models with `gpt-3.5-turbo`
    routing_strategy: least-busy      # Literal ["simple-shuffle", "least-busy", "usage-based-routing", "latency-based-routing"]
    num_retries: 2
    timeout: 60                       # seconds